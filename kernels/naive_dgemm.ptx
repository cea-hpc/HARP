//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	naive_dgemm

.visible .entry naive_dgemm(
	.param .u64 naive_dgemm_param_0,
	.param .u64 naive_dgemm_param_1,
	.param .u64 naive_dgemm_param_2,
	.param .f64 naive_dgemm_param_3,
	.param .u64 naive_dgemm_param_4,
	.param .u64 naive_dgemm_param_5,
	.param .u64 naive_dgemm_param_6,
	.param .u64 naive_dgemm_param_7,
	.param .f64 naive_dgemm_param_8,
	.param .u64 naive_dgemm_param_9,
	.param .u64 naive_dgemm_param_10
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<3>;
	.reg .f64 	%fd<35>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd28, [naive_dgemm_param_2];
	ld.param.f64 	%fd8, [naive_dgemm_param_3];
	ld.param.u64 	%rd33, [naive_dgemm_param_4];
	ld.param.u64 	%rd29, [naive_dgemm_param_5];
	ld.param.u64 	%rd34, [naive_dgemm_param_6];
	ld.param.u64 	%rd30, [naive_dgemm_param_7];
	ld.param.f64 	%fd9, [naive_dgemm_param_8];
	ld.param.u64 	%rd31, [naive_dgemm_param_9];
	ld.param.u64 	%rd32, [naive_dgemm_param_10];
	cvta.to.global.u64 	%rd1, %rd34;
	cvta.to.global.u64 	%rd2, %rd33;
	mov.u32 	%r1, %tid.y;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd4, %r2;
	setp.eq.s64 	%p1, %rd28, 0;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p1 bra 	$L__BB0_7;

	mul.lo.s64 	%rd5, %rd3, %rd29;
	and.b64  	%rd6, %rd28, 3;
	add.s64 	%rd36, %rd28, -1;
	setp.lt.u64 	%p2, %rd36, 3;
	mov.f64 	%fd34, 0d0000000000000000;
	mov.u64 	%rd58, 0;
	@%p2 bra 	$L__BB0_4;

	sub.s64 	%rd7, %rd6, %rd28;
	shl.b64 	%rd38, %rd4, 3;
	add.s64 	%rd56, %rd1, %rd38;
	shl.b64 	%rd39, %rd5, 3;
	add.s64 	%rd40, %rd2, %rd39;
	add.s64 	%rd55, %rd40, 16;
	shl.b64 	%rd10, %rd30, 3;

$L__BB0_3:
	ld.global.f64 	%fd14, [%rd56];
	ld.global.f64 	%fd15, [%rd55+-16];
	fma.rn.f64 	%fd16, %fd15, %fd14, %fd34;
	add.s64 	%rd41, %rd56, %rd10;
	ld.global.f64 	%fd17, [%rd41];
	ld.global.f64 	%fd18, [%rd55+-8];
	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
	add.s64 	%rd42, %rd41, %rd10;
	ld.global.f64 	%fd20, [%rd42];
	ld.global.f64 	%fd21, [%rd55];
	fma.rn.f64 	%fd22, %fd21, %fd20, %fd19;
	add.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd56, %rd43, %rd10;
	ld.global.f64 	%fd23, [%rd43];
	ld.global.f64 	%fd24, [%rd55+8];
	fma.rn.f64 	%fd34, %fd24, %fd23, %fd22;
	add.s64 	%rd58, %rd58, 4;
	add.s64 	%rd44, %rd7, %rd58;
	add.s64 	%rd55, %rd55, 32;
	setp.ne.s64 	%p3, %rd44, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s64 	%p4, %rd6, 0;
	@%p4 bra 	$L__BB0_7;

	mul.lo.s64 	%rd45, %rd58, %rd30;
	add.s64 	%rd46, %rd45, %rd4;
	shl.b64 	%rd47, %rd46, 3;
	add.s64 	%rd61, %rd1, %rd47;
	shl.b64 	%rd19, %rd30, 3;
	add.s64 	%rd48, %rd58, %rd5;
	shl.b64 	%rd49, %rd48, 3;
	add.s64 	%rd60, %rd2, %rd49;
	neg.s64 	%rd59, %rd6;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.f64 	%fd25, [%rd61];
	ld.global.f64 	%fd26, [%rd60];
	fma.rn.f64 	%fd34, %fd26, %fd25, %fd34;
	add.s64 	%rd61, %rd61, %rd19;
	add.s64 	%rd60, %rd60, 8;
	add.s64 	%rd59, %rd59, 1;
	setp.ne.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	fma.rn.f64 	%fd27, %fd34, %fd8, %fd9;
	mul.lo.s64 	%rd50, %rd3, %rd32;
	add.s64 	%rd51, %rd50, %rd4;
	cvta.to.global.u64 	%rd52, %rd31;
	shl.b64 	%rd53, %rd51, 3;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.f64 	%fd28, [%rd54];
	mul.f64 	%fd29, %fd27, %fd28;
	st.global.f64 	[%rd54], %fd29;
	ret;

}

