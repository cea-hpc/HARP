//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_75
.address_size 64

	// .globl	naive_sgemm

.visible .entry naive_sgemm(
	.param .u64 naive_sgemm_param_0,
	.param .u64 naive_sgemm_param_1,
	.param .u64 naive_sgemm_param_2,
	.param .f32 naive_sgemm_param_3,
	.param .u64 naive_sgemm_param_4,
	.param .u64 naive_sgemm_param_5,
	.param .u64 naive_sgemm_param_6,
	.param .u64 naive_sgemm_param_7,
	.param .f32 naive_sgemm_param_8,
	.param .u64 naive_sgemm_param_9,
	.param .u64 naive_sgemm_param_10
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd28, [naive_sgemm_param_2];
	ld.param.f32 	%f8, [naive_sgemm_param_3];
	ld.param.u64 	%rd33, [naive_sgemm_param_4];
	ld.param.u64 	%rd29, [naive_sgemm_param_5];
	ld.param.u64 	%rd34, [naive_sgemm_param_6];
	ld.param.u64 	%rd30, [naive_sgemm_param_7];
	ld.param.f32 	%f9, [naive_sgemm_param_8];
	ld.param.u64 	%rd31, [naive_sgemm_param_9];
	ld.param.u64 	%rd32, [naive_sgemm_param_10];
	cvta.to.global.u64 	%rd1, %rd34;
	cvta.to.global.u64 	%rd2, %rd33;
	mov.u32 	%r1, %tid.y;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd4, %r2;
	setp.eq.s64 	%p1, %rd28, 0;
	mov.f32 	%f34, 0f00000000;
	@%p1 bra 	$L__BB0_7;

	mul.lo.s64 	%rd5, %rd3, %rd29;
	and.b64  	%rd6, %rd28, 3;
	add.s64 	%rd36, %rd28, -1;
	setp.lt.u64 	%p2, %rd36, 3;
	mov.f32 	%f34, 0f00000000;
	mov.u64 	%rd58, 0;
	@%p2 bra 	$L__BB0_4;

	sub.s64 	%rd7, %rd6, %rd28;
	shl.b64 	%rd38, %rd4, 2;
	add.s64 	%rd56, %rd1, %rd38;
	shl.b64 	%rd39, %rd5, 2;
	add.s64 	%rd40, %rd2, %rd39;
	add.s64 	%rd55, %rd40, 8;
	shl.b64 	%rd10, %rd30, 2;

$L__BB0_3:
	ld.global.f32 	%f14, [%rd56];
	ld.global.f32 	%f15, [%rd55+-8];
	fma.rn.ftz.f32 	%f16, %f15, %f14, %f34;
	add.s64 	%rd41, %rd56, %rd10;
	ld.global.f32 	%f17, [%rd41];
	ld.global.f32 	%f18, [%rd55+-4];
	fma.rn.ftz.f32 	%f19, %f18, %f17, %f16;
	add.s64 	%rd42, %rd41, %rd10;
	ld.global.f32 	%f20, [%rd42];
	ld.global.f32 	%f21, [%rd55];
	fma.rn.ftz.f32 	%f22, %f21, %f20, %f19;
	add.s64 	%rd43, %rd42, %rd10;
	add.s64 	%rd56, %rd43, %rd10;
	ld.global.f32 	%f23, [%rd43];
	ld.global.f32 	%f24, [%rd55+4];
	fma.rn.ftz.f32 	%f34, %f24, %f23, %f22;
	add.s64 	%rd58, %rd58, 4;
	add.s64 	%rd44, %rd7, %rd58;
	add.s64 	%rd55, %rd55, 16;
	setp.ne.s64 	%p3, %rd44, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.eq.s64 	%p4, %rd6, 0;
	@%p4 bra 	$L__BB0_7;

	mul.lo.s64 	%rd45, %rd58, %rd30;
	add.s64 	%rd46, %rd45, %rd4;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd61, %rd1, %rd47;
	shl.b64 	%rd19, %rd30, 2;
	add.s64 	%rd48, %rd58, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd60, %rd2, %rd49;
	neg.s64 	%rd59, %rd6;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.f32 	%f25, [%rd61];
	ld.global.f32 	%f26, [%rd60];
	fma.rn.ftz.f32 	%f34, %f26, %f25, %f34;
	add.s64 	%rd61, %rd61, %rd19;
	add.s64 	%rd60, %rd60, 4;
	add.s64 	%rd59, %rd59, 1;
	setp.ne.s64 	%p5, %rd59, 0;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	fma.rn.ftz.f32 	%f27, %f34, %f8, %f9;
	mul.lo.s64 	%rd50, %rd3, %rd32;
	add.s64 	%rd51, %rd50, %rd4;
	cvta.to.global.u64 	%rd52, %rd31;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd54, %rd52, %rd53;
	ld.global.f32 	%f28, [%rd54];
	mul.ftz.f32 	%f29, %f27, %f28;
	st.global.f32 	[%rd54], %f29;
	ret;

}

